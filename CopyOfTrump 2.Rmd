---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


```{R}
library(reticulate)
```

```{python}
import numpy as np
import pandas as pd
import torch
from numpy.testing import assert_almost_equal
import re
import spacy
import string
from spacy.tokens import DocBin
import os
is_using_gpu = spacy.prefer_gpu()
if is_using_gpu:
    torch.set_default_tensor_type("torch.cuda.FloatTensor")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
```

```{python}
train=pd.read_csv('train.csv')
train['created_at']=pd.to_datetime(train['created_at']).dt.date
print(train.shape)
train=train[(train['is_retweet']==False) & (train['favorite_count']>0)]
print(train.shape)
print(train.head())
```

```{python}
train['created_at'].value_counts()
```

```{python}
train['importance']=(train['retweet_count']+train['favorite_count'])/2
train.drop(['retweet_count','favorite_count','is_retweet'],axis=1,inplace=True)
train['text']=train['text'].apply(lambda x: re.split('https:\/\/.*', str(x))[0]).str.replace('[{}]'.format(string.punctuation), '')
train=train[train['text']!=""]
train=train.groupby(['created_at']).apply(lambda x: x.nlargest(1,'importance')).reset_index(drop=True)
train.drop(['importance'],axis=1,inplace=True)
print(train.head())
print(train.shape)
```
```{Python}
from spacy_transformers import TransformersLanguage, TransformersWordPiecer, TransformersTok2Vec
name = "bert-base-uncased"
nlp = TransformersLanguage(trf_name=name, meta={"lang": "en"})
nlp.add_pipe(nlp.create_pipe("sentencizer"))
nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))
nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, name))
print(nlp.pipe_names) 
```


```{python}
nlp = spacy.load("en_trf_bertbaseuncased_lg")
cleaning= list(nlp.pipe(train['text'],n_threads=7))
train['score']=pd.Series([doc.vector_norm for doc in cleaning])
cleaning=[doc.text for doc in cleaning]
train['text']=pd.Series(cleaning)
train.head()
```
```{python}
train.shape
train.to_csv('train_data.csv',index=False)
```


```{R echo=F,message=F}
library(tidyverse)
library(reticulate)
library(quantmod)
library(rtweet)
library(data.table)
library(lubridate)
library(e1071)
library(doParallel)
cl<-makeCluster(7)
registerDoParallel(cl)
future::plan('multicore')
library(DT)
library(feasts)
library(caret)
library(fable)
library(tsibble)
```

```{R , warning=F,message=F}
invisible(getSymbols('SPY',src='yahoo',from='2018-01-10',verbose=F))
chartSeries(SPY)
```

```{R message=F,tidy=T}
data<-fread('train_data.csv',select=c('created_at','score'),col.names=c('date','score'))
prices<-fread('SPY.csv',col.names = c('date','price'))
prices$date<-mdy(prices$date)
data$date<-ymd(data$date)
data<-right_join(data,prices,by='date')
data<-as_tsibble(data,index=date)  %>% tsibble::fill_gaps(.full = T) %>% fill(price,.direction='down') %>% fill(score,.direction = 'down') %>% filter(score>0)
lambda<-(data %>% features(price,guerrero))[[1]]
data$price<-box_cox(data$price,lambda)
train<-as_tsibble(data[1:round(nrow(data)*.95-1,0),],index=date)
test<-as_tsibble(data[round(nrow(data)*.95-1,0):nrow(data),],index=date)
head(data)
```
```{R}
data %>% gg_tsdisplay(price,plot_type='partial')
```



```{R}
fit<-train %>% model(norm=ARIMA(price~trend()),lag0=ARIMA(price~score+trend()),lag1=ARIMA(price~score+lag(score,1)+trend()),lag2=ARIMA(price~score+lag(score,1)+lag(score,2)+trend())
                    ,lag3=ARIMA(price~score+lag(score,1)+lag(score,2)+lag(score,4)+trend()),lag4=ARIMA(price~score+lag(score,1)+lag(score,2)+lag(score,4)+lag(score,6)+trend()),nn=NNETAR(price~score))
glance(fit)
```

```{R}
fit_best<-train %>% model(ARIMA(price~score+lag(score,1)+lag(score,2)+lag(score,4)+lag(score,6)+trend()))
report(fit_best)
```
```{R}
gg_arma(fit_best)
```

```{R}
fit_best %>% gg_tsresiduals()
```

```{R}
fit_best %>% forecast(test[,1:2]) %>% autoplot(test)
```