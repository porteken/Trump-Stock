---
title: "R Notebook"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---


```{R message=F}
library(tidyverse)
library(reticulate)
library(quantmod)
library(rtweet)
library(data.table)
library(lubridate)
library(e1071)
library(doParallel)
cl<-makeCluster(7)
registerDoParallel(cl)
future::plan('multicore')
library(DT)
library(feasts)
library(caret)
library(fable)
library(data.table)
library(tsibble)
```

```{R}
data<-fread('train.csv')
data$created_at<-with_tz(parse_date_time(data$created_at,'%m/%d/%Y H:M'),tzone='US/Central')
dim(data)
data<-data %>% filter(is_retweet==F & favorite_count>0) %>% select(-one_of('is_retweet')) %>% mutate(date=if_else(am(created_at),date(created_at),date(created_at+days(1)))) %>% select(-one_of('created_at'))
dim(data)
```
```{R}
data$text<-gsub('http\\S+\\s*','',data$text)
data$text<-gsub("[^0-9A-Za-z///' ]",'',data$text,ignore.case = T)
datatable(head(data))
```

```{R}
data<-data %>% mutate(importance=(retweet_count+favorite_count)/2) %>% select(text,date,importance)
data<-data %>% filter(text!='') %>% group_by(date) %>% top_n(1,importance) %>% select(-one_of('importance'))
```

```{python}
import numpy as np
import pandas as pd
import torch
from numpy.testing import assert_almost_equal
import re
import spacy
import string
from spacy.tokens import DocBin
import os
is_using_gpu = spacy.prefer_gpu()
if is_using_gpu:
    torch.set_default_tensor_type("torch.cuda.FloatTensor")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
```

```{Python}
from spacy_transformers import TransformersLanguage, TransformersWordPiecer, TransformersTok2Vec
name = "bert-base-uncased"
nlp = TransformersLanguage(trf_name=name, meta={"lang": "en"})
nlp.add_pipe(nlp.create_pipe("sentencizer"))
nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))
nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, name))
print(nlp.pipe_names) 
```


```{python}
trains=r.data
nlp = spacy.load("en_trf_bertbaseuncased_lg")
cleaning= list(nlp.pipe(trains['text'],n_threads=16, batch_size=10000))
trains['score']=pd.Series([doc.vector_norm for doc in cleaning])
cleaning=[doc.text for doc in cleaning]
trains['text']=pd.Series(cleaning)
trains.head()
```

```{python}
trains.shape
trains['date']=pd.to_datetime(trains['date'])
print(trains.head())
```


```{R message=F,tidy=T}
data<-py$trains
data$date<-date(data$date)
prices<-fread('SPY.csv',col.names = c('date','price'))
prices$date<-mdy(prices$date)
data<-right_join(data,prices,by='date')
data<-as_tsibble(data,index=date)  %>% tsibble::fill_gaps(.full = T) %>% fill(price,.direction='down') %>% fill(score,.direction = 'down') %>% filter(score>0)
train<-as_tsibble(data[1:round(nrow(data)*.95-1,0),],index=date)
test<-as_tsibble(data[round(nrow(data)*.95-1,0):nrow(data),],index=date)
data.table(data)
```

```{R}
data %>% gg_tsdisplay(price,plot_type='partial')
```



```{R}
fit<-train %>% model(norm=ARIMA(price~trend()),lag0=ARIMA(price~score+trend()),lag1=ARIMA(price~score+lag(score,1)+trend()),lag2=ARIMA(price~score+lag(score,1)+lag(score,2)+trend())
                    ,lag3=ARIMA(price~score+lag(score,1)+lag(score,2)+lag(score,4)+trend()),lag4=ARIMA(price~score+lag(score,1)+lag(score,2)+lag(score,4)+lag(score,6)+trend()),nn=NNETAR(price~score))
glance(fit)
```

```{R}
fit_best<-train %>% model(ARIMA(price~score+lag(score,1)+lag(score,2)+lag(score,4)+lag(score,6)+trend()))
report(fit_best)
```
```{R}
gg_arma(fit_best)
```

```{R}
fit_best %>% gg_tsresiduals()
```

```{R}
fit_best %>% forecast(test) %>% autoplot(test)
```